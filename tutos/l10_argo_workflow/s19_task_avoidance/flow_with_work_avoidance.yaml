##################### work avoidance. #######################################
# A workflow that utilizes this is simply a workflow containing steps that do not run if the work has already
# been done. This simplest way to do this is to use marker files.
# Marker files are just empty files with the name of the specific task name to indicate that the task is done
# and no need to re-run it again.
# Use cases:
# - An expensive step appears across multiple workflows - you want to avoid repeating them.
# - A workflow has unreliable tasks - you want to be able resubmit the workflow.
#
# Note the marker file name must be unique for each task. The standard way is to use the template name, execution-date
# and all the parameters of the template. If the task need to be renew every our, you can also change execution-date to
# execution-hour. Or just omit it, if the task only need to run once. In our example, the template name is runner, so
# we have the marker file as $(date +%Y-%m-%d)-runner-{{inputs.parameters.num}}

# This example shows how to use a PVC and optional input artifacts to avoid work.
# step 1: load-marker task read markers file from s3 artifact and write them to pvc
# Step 2: Check first if the job is done or not(by checking if a marker file exists or not).
#         if yes, skip job. if not, do the job(echo "working very hard"),and create file marker
#         based on date and parameter
# onExit step: write the marker files to s3
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: flow-with-work-avoidance-
spec:
  entrypoint: main
# Create a pvc for the workflow
  volumeClaimTemplates:
    - metadata:
        name: work
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 10Mi

  onExit: save-markers
# workflow has two main step, load-marker and echo
  templates:
    - name: main
      steps:
        - - name: load-markers
            template: load-markers
        - - name: run-tasks
            template: runner
            arguments:
              parameters:
                - name: num
                  value: "{{item}}"
            withSequence:
              count: "3"
# step 1: load-marker read file markers from s3 artifact and write them to pvc
    - name: load-markers
      inputs:
        artifacts:
          - name: markers
            path: /work/markers
            optional: true
            s3:
              key: work-avoidance-markers/markers.tgz
              bucket: argo-artifacts
              endpoint: minio.lab.sspcloud.fr
              accessKeySecret:
                name: prod-minio-cred
                key: accessKey
              secretKeySecret:
                name: prod-minio-cred
                key: secretKey
      container:
        image: docker/whalesay:latest
        command:
          - mkdir
          - -p
          - /work/markers
        volumeMounts:
          - name: work
            mountPath: /work
# Step 2: Check first if the job is done or not(by checking if a marker file exists or not).
#         if yes, skip job. if not, do the job(echo "working very hard"),and create file marker
#         based on date and parameter
    - name: runner
      inputs:
        parameters:
          - name: num
      script:
        image: docker/whalesay:latest
        command:
          - bash
          - -eux
        source: |
          marker=/work/markers/$(date +%Y-%m-%d)-runner-{{inputs.parameters.num}}
          if [ -e  ${marker} ]; then
            echo "job already done"
            exit 0
          fi
          echo "working very hard"
          # toss a virtual coin and exit 1 if 1
          if [ $(($(($RANDOM%10))%2)) -eq 1 ]; then
            echo "job failed!"
            exit 1
          fi
          touch ${marker}
        volumeMounts:
          - name: work
            mountPath: /work
    # onExit step write the file marker to s3
    - name: save-markers
      container:
        image: docker/whalesay:latest
        command:
          - "true"
        volumeMounts:
          - name: work
            mountPath: /work
      outputs:
        artifacts:
          - name: markers
            path: /work/markers
            s3:
              # Note the key is very important, even marker is a directory in pvc, when we output it to s3
              # it will be compressed into a tgz. And we must write down the complete path in key with the
              # compressed filename of the directory
              # In this example. all the content of /work/markers/... will be compressed into marker.tgz
              # and output to s3://minio.lab.sspcloud.fr/argo-artifacts/work-avoidance-markers/markers.tgz
              # If we only put work-avoidance-marker in key, we will have the following error.
              # Failed to put file: Please reduce your request
              key: work-avoidance-markers/markers.tgz
              bucket: argo-artifacts
              endpoint: minio.lab.sspcloud.fr
              accessKeySecret:
                name: prod-minio-cred
                key: accessKey
              secretKeySecret:
                name: prod-minio-cred
                key: secretKey