apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: flow-with-test-artifact-
spec:
  entrypoint: artifact-example
  templates:
    - name: artifact-example
      inputs:
        artifacts:
        - name: my-input-artifact
          path: /my-input-artifact
          s3:
            endpoint: 10.233.22.96:9000
            bucket: argo-repo
            key: my-input-artifact.tgz
            accessKeySecret:
              name: pengfei-private-minio-secret
              key: accessKey
            secretKeySecret:
              name: pengfei-private-minio-secret
              key: secretKey
      volumes:
        - name: out
          emptyDir: { }
      outputs:
        artifacts:
        - name: my-output-artifact
          path: /mnt/out/my-output-artifact
          s3:
            endpoint: 10.233.22.96:9000
            bucket: argo-repo
          # NOTE that, by default, all output artifacts are automatically tarred and
          # gzipped before saving. So as a best practice, .tgz or .tar.gz
          # should be incorporated into the key name so the resulting file
          # has an accurate file extension.
            key: my-output-artifact.tgz
            accessKeySecret:
              name: pengfei-private-minio-secret
              key: accessKey
            secretKeySecret:
              name: pengfei-private-minio-secret
              key: secretKey
            #region: my-GCS-storage-bucket-region
      container:
        image: debian:latest
        command: ["cat","hello world", ">>", "/mnt/out/hello_world.txt"]
        # args: ["cp -r /my-input-artifact /my-output-artifact"]
        volumeMounts:
        - name: out
          mountPath: /mnt/out